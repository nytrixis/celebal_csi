{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1315bf41",
   "metadata": {},
   "source": [
    "# 🤖 Model Training and Evaluation\n",
    "## House Price Prediction - Machine Learning Pipeline\n",
    "\n",
    "Welcome to the final notebook in our house price prediction series! In this notebook, we'll:\n",
    "\n",
    "1. **Load Engineered Features** - Import the processed data from our feature engineering\n",
    "2. **Train Multiple Models** - Test various ML algorithms\n",
    "3. **Evaluate Performance** - Compare models using multiple metrics  \n",
    "4. **Hyperparameter Tuning** - Optimize the best performing models\n",
    "5. **Make Predictions** - Generate predictions on test data\n",
    "6. **Model Interpretation** - Understand feature importance\n",
    "\n",
    "Let's build some powerful predictive models! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02e175",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing our necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports with fallback handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📦 Basic packages loaded successfully!\")\n",
    "\n",
    "# Try to import scikit-learn components\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"✅ Scikit-learn imported successfully!\")\n",
    "    sklearn_available = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Scikit-learn import error: {e}\")\n",
    "    print(\"🔄 Will use basic implementations where possible\")\n",
    "    sklearn_available = False\n",
    "\n",
    "# Try to import additional useful libraries\n",
    "try:\n",
    "    import scipy.stats as stats\n",
    "    print(\"✅ SciPy available for statistical functions\")\n",
    "    scipy_available = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ SciPy not available - using basic statistics\")\n",
    "    scipy_available = False\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n🎯 Environment setup complete!\")\n",
    "print(f\"   Scikit-learn: {'✅' if sklearn_available else '❌'}\")\n",
    "print(f\"   SciPy: {'✅' if scipy_available else '❌'}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735343c",
   "metadata": {},
   "source": [
    "## 2. Load Engineered Data\n",
    "\n",
    "Let's load the features we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Engineered Data\n",
    "print(\"=== Loading Engineered Data ===\")\n",
    "\n",
    "# Define paths to our processed data\n",
    "data_dir = \"../outputs/processed_data\"\n",
    "original_data_dir = \"../data\"\n",
    "\n",
    "# Try to load engineered features first\n",
    "try:\n",
    "    # Load the complete engineered dataset\n",
    "    engineered_file = os.path.join(data_dir, \"engineered_features.csv\")\n",
    "    if os.path.exists(engineered_file):\n",
    "        df = pd.read_csv(engineered_file)\n",
    "        print(f\"✅ Loaded engineered features from: {engineered_file}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        data_source = \"engineered\"\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Engineered features not found\")\n",
    "        \n",
    "except (FileNotFoundError, Exception) as e:\n",
    "    print(f\"⚠️ Could not load engineered features: {e}\")\n",
    "    print(\"🔄 Trying to load original training data...\")\n",
    "    \n",
    "    # Fallback to original data\n",
    "    try:\n",
    "        train_file = os.path.join(original_data_dir, \"train.csv\")\n",
    "        if os.path.exists(train_file):\n",
    "            df = pd.read_csv(train_file)\n",
    "            print(f\"✅ Loaded original training data from: {train_file}\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            data_source = \"original\"\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No data files found\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Could not load any data: {e2}\")\n",
    "        print(\"Please make sure you have either:\")\n",
    "        print(\"1. Run the feature engineering notebook first, OR\")\n",
    "        print(\"2. Downloaded the Kaggle data to ../data/\")\n",
    "        data_source = None\n",
    "\n",
    "if data_source:\n",
    "    # Basic data info\n",
    "    print(f\"\\n📊 Dataset Information:\")\n",
    "    print(f\"   Rows: {df.shape[0]:,}\")\n",
    "    print(f\"   Columns: {df.shape[1]:,}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Check for target variable\n",
    "    if 'SalePrice' in df.columns:\n",
    "        print(f\"   Target variable: SalePrice\")\n",
    "        print(f\"   Target range: ${df['SalePrice'].min():,.0f} - ${df['SalePrice'].max():,.0f}\")\n",
    "        print(f\"   Target mean: ${df['SalePrice'].mean():,.0f}\")\n",
    "        has_target = True\n",
    "    else:\n",
    "        print(\"   ⚠️ No target variable found\")\n",
    "        has_target = False\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    print(f\"   Missing values: {missing_count}\")\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        print(\"   Features with missing values:\")\n",
    "        missing_features = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for feature, count in missing_features.head(10).items():\n",
    "            print(f\"     {feature}: {count}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\n📋 First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\n🔢 Data types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot proceed without data. Please check your data files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f4264",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Now let's prepare our data for machine learning by creating features (X) and target (y) variables, and splitting into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "print(\"=== Data Preparation ===\")\n",
    "\n",
    "if data_source and has_target:\n",
    "    # Prepare features and target\n",
    "    print(\"Preparing features and target variable...\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    if 'SalePrice' in df.columns:\n",
    "        X = df.drop('SalePrice', axis=1)\n",
    "        y = df['SalePrice']\n",
    "        print(f\"✅ Features (X): {X.shape}\")\n",
    "        print(f\"✅ Target (y): {y.shape}\")\n",
    "    else:\n",
    "        print(\"❌ No target variable found\")\n",
    "        X, y = None, None\n",
    "    \n",
    "    if X is not None and y is not None:\n",
    "        # Handle missing values in features\n",
    "        print(f\"\\n🧹 Cleaning data...\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_cols = X.isnull().sum()\n",
    "        missing_cols = missing_cols[missing_cols > 0]\n",
    "        \n",
    "        if len(missing_cols) > 0:\n",
    "            print(f\"   Found {len(missing_cols)} columns with missing values\")\n",
    "            \n",
    "            # Fill missing values\n",
    "            for col in missing_cols.index:\n",
    "                if X[col].dtype in ['int64', 'float64']:\n",
    "                    # Fill numerical columns with median\n",
    "                    X[col].fillna(X[col].median(), inplace=True)\n",
    "                else:\n",
    "                    # Fill categorical columns with mode\n",
    "                    X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "            \n",
    "            print(f\"   ✅ Filled missing values\")\n",
    "        else:\n",
    "            print(f\"   ✅ No missing values found\")\n",
    "        \n",
    "        # Handle infinite values\n",
    "        inf_cols = []\n",
    "        for col in X.select_dtypes(include=[np.number]).columns:\n",
    "            if np.isinf(X[col]).any():\n",
    "                inf_cols.append(col)\n",
    "                X[col].replace([np.inf, -np.inf], X[col].median(), inplace=True)\n",
    "        \n",
    "        if inf_cols:\n",
    "            print(f\"   ✅ Handled infinite values in {len(inf_cols)} columns\")\n",
    "        \n",
    "        # Convert categorical variables to numerical if needed\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"   🏷️ Found {len(categorical_cols)} categorical columns\")\n",
    "            \n",
    "            # Simple label encoding for categorical variables\n",
    "            for col in categorical_cols:\n",
    "                unique_vals = X[col].unique()\n",
    "                if len(unique_vals) <= 50:  # Only encode if not too many categories\n",
    "                    label_map = {val: i for i, val in enumerate(unique_vals)}\n",
    "                    X[col] = X[col].map(label_map)\n",
    "                else:\n",
    "                    print(f\"     Dropping {col} (too many categories: {len(unique_vals)})\")\n",
    "                    X = X.drop(col, axis=1)\n",
    "            \n",
    "            print(f\"   ✅ Encoded categorical variables\")\n",
    "        \n",
    "        # Final check\n",
    "        print(f\"\\n📊 Final dataset info:\")\n",
    "        print(f\"   Features shape: {X.shape}\")\n",
    "        print(f\"   Target shape: {y.shape}\")\n",
    "        print(f\"   Feature types: {X.dtypes.value_counts().to_dict()}\")\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        if sklearn_available:\n",
    "            print(f\"\\n✂️ Splitting data...\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "            print(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "            \n",
    "            # Basic statistics\n",
    "            print(f\"\\n📈 Target variable statistics:\")\n",
    "            print(f\"   Training mean: ${y_train.mean():,.0f}\")\n",
    "            print(f\"   Training std: ${y_train.std():,.0f}\")\n",
    "            print(f\"   Test mean: ${y_test.mean():,.0f}\")\n",
    "            print(f\"   Test std: ${y_test.std():,.0f}\")\n",
    "            \n",
    "            data_ready = True\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️ Scikit-learn not available - manual split needed\")\n",
    "            # Manual train/test split\n",
    "            split_idx = int(0.8 * len(X))\n",
    "            indices = np.random.permutation(len(X))\n",
    "            \n",
    "            train_idx = indices[:split_idx]\n",
    "            test_idx = indices[split_idx:]\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            print(f\"   ✅ Manual split completed\")\n",
    "            print(f\"   Training set: {len(X_train)} samples\")\n",
    "            print(f\"   Test set: {len(X_test)} samples\")\n",
    "            \n",
    "            data_ready = True\n",
    "    else:\n",
    "        data_ready = False\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot prepare data without target variable\")\n",
    "    data_ready = False\n",
    "\n",
    "if data_ready:\n",
    "    print(f\"\\n🎉 Data preparation complete!\")\n",
    "    print(f\"   Ready for model training!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Data preparation failed\")\n",
    "    print(f\"   Please check your data and try again\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
